{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **1. Libraries and Settings**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport matplotlib.pyplot as plt\nimport math, time\nfrom math import sqrt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport random\nimport itertools\nimport datetime\nfrom operator import itemgetter\n\n# List all files under the input directory from kaggle\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2. Analyze data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def stocksData(symbols, dates):\n    df = pd.DataFrame(index=dates)\n    for symbol in symbols:\n        dfTemp = pd.read_csv(\"/kaggle/input/price-volume-data-for-all-us-stocks-etfs/Stocks/{}.us.txt\".format(symbol), index_col='Date',\n                parse_dates=True, usecols=['Date', 'Close'], na_values=['NaN'])\n        dfTemp = dfTemp.rename(columns={'Close': symbol})\n        # Add the column to the DataFrame:\n        df = df.join(dfTemp)\n    return df\n                              \n# freq ‘B’ = business daily:\ndates = pd.date_range('2015-01-02','2016-12-31',freq='B')\nsymbols = ['goog','ibm','aapl']\ndf = stocksData(symbols, dates)\n# method 'pad': fill values forward(propagate last valid observation forward to next valid backfill):\ndf.fillna(method='pad')\nprint(df)\ndf.interpolate().plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = pd.date_range('2010-01-02','2017-10-11',freq='B')\ndf1 = pd.DataFrame(index=dates)\ndfIbm = pd.read_csv(\"/kaggle/input/price-volume-data-for-all-us-stocks-etfs/Stocks/ibm.us.txt\", parse_dates=True, index_col=0)\ndfIbm = df1.join(dfIbm)\ndfIbm = dfIbm[['Close']]\ndfIbm.plot()\nplt.ylabel(\"Stock Price\")\nplt.title(\"IBM Stock\")\nplt.show()\ndfIbm.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'ffill'is like'pad': fill values forward(propagate last valid observation forward to next valid backfill):\ndfIbm = dfIbm.fillna(method='ffill')\n\n# MinMaxScaler: transform features by scaling each feature to a given range\nscaler = MinMaxScaler(feature_range=(-1,1))\n# reshape(-1,1): We have provided column as 1 but rows as unknown(numpy will figure out)\ndfIbm['Close'] = scaler.fit_transform(dfIbm['Close'].values.reshape(-1,1))\ndfIbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# Function that creates train and test data through stock data and sequence lenght\ndef loadData(stock, lookBack):\n    data = []\n    # Convert to numpy array\n    dataRaw = stock.values\n    \n    # Create all possible sequences of length sequence length\n    for i in range(len(dataRaw)-lookBack):\n        data.append(dataRaw[i: i+lookBack])\n        \n    data = np.array(data)\n    testSize = int(np.round(0.2*data.shape[0]))\n    trainSize = data.shape[0]-testSize\n    \n    X_train = data[:trainSize,:-1,:]\n    y_train = data[:trainSize,-1,:]\n    X_test = data[trainSize:,:-1]\n    y_test = data[trainSize:,-1,:]\n    \n    return X_train, X_test, y_train, y_test    \n    \n# Sequence length:\nlookBack = 20\nX_train, X_test, y_train, y_test = loadData(dfIbm, lookBack)\nprint(\"X_train and y_train shapes:\", X_train.shape, y_train.shape)\nprint(\"X_test and y_test shapes:\", X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and test sets in Pytorch(turning into tensors)\nX_train = torch.from_numpy(X_train).type(torch.Tensor)\nX_test = torch.from_numpy(X_test).type(torch.Tensor)\ny_train = torch.from_numpy(y_train).type(torch.Tensor)\ny_test = torch.from_numpy(y_test).type(torch.Tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.size(), y_train.size(), X_test.size(), y_test.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stepsNumb = lookBack-1\n# Batch: number of training examples utilized in which(one) iteration of the epochs\nbatchSize = 1606\n# Epoch: the number of passes into the entire training dataset\nnumEpochs = 100\n\n# Training and test dataset with torch:\n\ntrain = torch.utils.data.TensorDataset(X_train,y_train)\ntest = torch.utils.data.TensorDataset(X_test,y_test)\n\ntrainLoader = torch.utils.data.DataLoader(dataset=train, batch_size=batchSize, shuffle=False)\ntestLoader = torch.utils.data.DataLoader(dataset=test, batch_size=batchSize,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. Build the structure of the Model**"},{"metadata":{},"cell_type":"markdown","source":"**LSTM(Long Short Term Memory) network**: A type of Recurrent Neural Network(RNN) capable of learning long-term dependencies. Usefull when the problem needs context. Designed for applications where the input is an ordered sequence where information from earlier in the sequence may be important. The nodes are recurrent but they also have an internal state as a working memory space(information can be stored and retrieved). As any other neural network, has nodes with paramethers(called gates) that are learned during training. It is a Gated Recurrent Network, that the network decide what to remember and what to forget by introducing new parameters that act as gates.\n\nRNN's are networks that reuse the output from a previus step as an input for the next step, they have loops in them, allowing information to persist. A RNN can be thought of as multiple copies of the same network, each passing a message to a successor.\n\nBasically, every hidden unit of the rnn is replaced with something called an LSTM Cell, and another connection is added from every cell called cell state. Each LSTM cell maintain a cell state vector and at each time step the next LSTM can choose to read from it right to it or reset the cell using an explicit gating mechanism.\n\nLonger sequences in traditional RNN cause exploding/vanishing gradients. LSTM/GRU deal with such longer sequences. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build model\n\n# Hyperparameters\ninputDim = 1\nhiddenDim = 32\nnumLayers = 2\noutputDim = 1\n\n# Defining the model as a class\n# nn.Module: base class for all neural network modules, your models should also subclass this class\nclass LSTM(nn.Module):\n    def __init__(self, inputDim, hiddenDim, numLayers, outputDim):\n        super(LSTM, self).__init__()\n        # Hidden dimensions\n        self.hiddenDim = hiddenDim\n        # Number of hidden layers\n        self.numLayers = numLayers\n        \n        # Building your LSTM\n        # batch_first=True: if input and output tensors are provided as (batch, seq, feature): the batch is the first access\n        self.lstm = nn.LSTM(inputDim, hiddenDim, numLayers, batch_first=True)\n        \n        # Fully connected, Readout layer(parameters of the final non-recurrent output layer)\n        self.fc = nn.Linear(hiddenDim, outputDim)\n        \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        # requires_grad_() = allows for fine grained exclusion of subgraphs from gradient computation and can increase efficiency\n        # Gradient: is another word for \"slope\"\n        # x.size(0): number of examples sent into the batch size\n        h0 = torch.zeros(self.numLayers, x.size(0), self.hiddenDim).requires_grad_().to(device)\n\n        # Initialize cell state\n        # Cell state: horizontal line running through the top of the diagram. It runs straight down the entire \n        ## chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\n        c0 = torch.zeros(self.numLayers, x.size(0), self.hiddenDim).requires_grad_().to(device)\n\n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        # datach(): returns a new Tensor, detached from the current graph. The result will never require gradient\n        out, (hiddenState, cellState) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Hidden state index of last time step\n        # out.size() --> 100, 28, 100\n        out = self.fc(out[:, -1, :]) # --> 100, 100\n\n        # out.size() --> 100, 10\n        return out\n        \nmodel = LSTM(inputDim=inputDim, hiddenDim=hiddenDim, numLayers=numLayers, outputDim=outputDim)\n\n# MSE: Creates a criterion that measures the mean squared error between each element in the input x and target y\nlossFn = torch.nn.MSELoss()\n\n# Optimiser: will hold the current state and will update the parameters based on the computed gradients\n# lr: learning rate\noptimiser = torch.optim.Adam(model.parameters(), lr=0.01)\nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **4. Train model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model\n\ntrainLoss = np.zeros(numEpochs)\n\n# Number of steps to unroll\nseqDim = lookBack-1\n\nfor e in range(numEpochs):\n    # Forward pass\n    yTrainPred = model(X_train)\n    \n    loss = lossFn(yTrainPred, y_train)\n    if e % 10 == 0 and e != 0:\n        print(\"Epoch: \", e, \"MSE: \", loss.item())\n    trainLoss[e] = loss.item()\n    \n    # Zero out gradient, else they will accumulate between epochs\n    optimiser.zero_grad()\n    \n    # Backwark pass\n    loss.backward()\n    \n    # Update parameters\n    optimiser.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"yTrainPred shape: \", np.shape(yTrainPred))\n\nplt.plot(yTrainPred.detach().numpy(), label=\"Preds\")\nplt.plot(y_train.detach().numpy(), label=\"Data\")\nplt.legend()\nplt.show()\n\nplt.plot(trainLoss, label=\"Training Loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **5. Make predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\nyTestPred = model(X_test)\n\n# Invert predictions(scale back the data to the original representation)\nyTrainPred = scaler.inverse_transform(yTrainPred.detach().numpy())\ny_train = scaler.inverse_transform(y_train.detach().numpy())\nyTestPred = scaler.inverse_transform(yTestPred.detach().numpy())\ny_test = scaler.inverse_transform(y_test.detach().numpy())\n\n# Calculate Root Mean Squared Error\ntrainScore = math.sqrt(mean_squared_error(yTrainPred[:,0], y_train[:,0]))\ntestScore = math.sqrt(mean_squared_error(yTestPred[:,0], y_test[:,0]))\nprint('Train Score: ', trainScore,'RMSE')\nprint('Test Score: ', testScore,'RMSE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shift train predictions for plotting\n# np.empty_like: returns a new array with the same shape and type as a given array\ntrainPredictPlot = np.empty_like(dfIbm)\ntrainPredictPlot[:,:] = np.nan\ntrainPredictPlot[lookBack:len(yTrainPred)+lookBack,:] = yTrainPred\n\n# Shift test predictions for plotting\ntestPredictPlot = np.empty_like(dfIbm)\ntestPredictPlot[:,:] = np.nan\ntestPredictPlot[len(yTrainPred)+lookBack-1:len(dfIbm)-1,:] = yTestPred\n\n# Plot baseline and predictions\nplt.figure(figsize=(15,8))\nplt.plot(scaler.inverse_transform(dfIbm))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}